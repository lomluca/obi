# OBI Master

## Code Structure
 - `master/autoscaler` code written for the autoscaler feature
 - `master/heartbeat` set of scripts and `protobuf` schemas used to run the
   heartbeat service on clusters master node and to collect them into OBI
   architecture
 - `master/model` definition of generic data structure, which define how each
   cluster should be interfaced
 - `master/persistent` code written to allow OBI to connect to a database for
   persisting its state
 - `master/platforms` implementations of the generic interfaces from
   `master/model`, capable of extending OBI to each possible cloud computing
   service
 - `master/pool` code allowing the handling of multiple cluster along with
   utility functions to allocate them
 - `master/predictor` contains code which is autogenerated to allow
   communication between OBI Master and the predictor component
 - `master/scheduling` contains the logic for the OBI scheduler
 - `master/utils` general utility functions


## Configuration
In the `values.yaml` inside the Helm chart folder there is the `masterConfig` map,
where the administrator have to set infos about the Google Cloud project and
settings about the scheduler. Specifically:
 - `projectId` the project id used in the Google Cloud Platform
 - `region` the region used for Google Cloud Dataproc
 - `zone` the zone used for Google Cloud Dataproc
 - `dataproc-image` the custom image used for Google Cloud Dataproc
 - `heartbeatHost` private IP address of any Kubernetes node, used by master
    nodes of Dataproc cluster to send hearbeat to a NodePort service
 - `schedulingLevels` the levels of the scheduler (from the lowest to the highest one)
    More information in the next section.

## Scheduler overview and configuration
In a cloud-based environment, we have to rethink our approach about job submission: 
we don’t have long-living on-premise clusters. We work with the so-called 
“ephemeral model”: only when the user needs to submit jobs, he will create a specific
cluster on the cloud, submit the jobs and delete it when every task has finished. 
In this way, we pay for what we actually use. 

The main goal of the OBI scheduler is grouping together jobs submitted by different 
users in the same time period, in order to reduce the number of spawned cluster. 
The scheduler can accommodate many little jobs in a single cluster or one heavy 
and few light ones, for example. In this way the module does not affect too much 
the performances of each single job.

These are the main concepts of the OBI scheduler:
- There are N priority level
- For each level, we have many bins
- One bin contains many job
- Upon job submission, the new size of the current bin is checked: if it exceeds 
  the threshold, another empty bin is added in the level
- When the level-timeout expires, all the jobs in a bin will be deployed in single cluster

OBI scheduler supports two scheduling policies while packaging jobs into bins:
 - **count based**: bin are filled with jobs coming from the same priority band
   up to a certain count
 - **time based**: in this policy the scheduler asks the predictor module to
   generate an estimation of how long a certain job will last and then tries to
   pack jobs with the same priority into homogenous bins e.g. each bin should
   contain jobs for a maximum total duration of 1 hour

This is an example of configuration for the scheduler:
```
schedulingLevels:
# level 0
- timeout: 180
  policy: 0
  binCapacity: 3600
# level 1
- timeout: 300
  policy: 1
  binCapacity: 3
# level 2 is one job one cluster
# level 3 is one job one High Performance cluster
```

In every case the two highest level are the "one job one cluster", with high-performance
or not. All the lower level can be fully customizable. The policy `0` is the
`time-based`, the policy `1` is the `count-based`.

